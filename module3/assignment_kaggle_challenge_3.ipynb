{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science, Unit 2: Predictive Modeling\n",
    "\n",
    "# Kaggle Challenge, Module 3\n",
    "\n",
    "\n",
    "## Assignment\n",
    "- [ ] [Review requirements for your portfolio project](https://lambdaschool.github.io/ds/unit2), then submit your dataset.\n",
    "- [ ] Continue to participate in our Kaggle challenge. \n",
    "- [ ] Use scikit-learn for hyperparameter optimization with RandomizedSearchCV.\n",
    "- [ ] Submit your predictions to our Kaggle competition. (Go to our Kaggle InClass competition webpage. Use the blue **Submit Predictions** button to upload your CSV file. Or you can use the Kaggle API to submit your predictions.)\n",
    "- [ ] Commit your notebook to your fork of the GitHub repo.\n",
    "\n",
    "## Stretch Goals\n",
    "\n",
    "### Reading\n",
    "- Jake VanderPlas, [Python Data Science Handbook, Chapter 5.3](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html), Hyperparameters and Model Validation\n",
    "- Jake VanderPlas, [Statistics for Hackers](https://speakerdeck.com/jakevdp/statistics-for-hackers?slide=107)\n",
    "- Ron Zacharski, [A Programmer's Guide to Data Mining, Chapter 5](http://guidetodatamining.com/chapter5/), 10-fold cross validation\n",
    "- Sebastian Raschka, [A Basic Pipeline and Grid Search Setup](https://github.com/rasbt/python-machine-learning-book/blob/master/code/bonus/svm_iris_pipeline_and_gridsearch.ipynb)\n",
    "- Peter Worcester, [A Comparison of Grid Search and Randomized Search Using Scikit Learn](https://blog.usejournal.com/a-comparison-of-grid-search-and-randomized-search-using-scikit-learn-29823179bc85)\n",
    "\n",
    "### Doing\n",
    "- In additon to `RandomizedSearchCV`, scikit-learn has [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Another library called scikit-optimize has [`BayesSearchCV`](https://scikit-optimize.github.io/notebooks/sklearn-gridsearchcv-replacement.html). Experiment with these alternatives.\n",
    "- _[Introduction to Machine Learning with Python](http://shop.oreilly.com/product/0636920030515.do)_ discusses options for \"Grid-Searching Which Model To Use\" in Chapter 6:\n",
    "\n",
    "> You can even go further in combining GridSearchCV and Pipeline: it is also possible to search over the actual steps being performed in the pipeline (say whether to use StandardScaler or MinMaxScaler). This leads to an even bigger search space and should be considered carefully. Trying all possible solutions is usually not a viable machine learning strategy. However, here is an example comparing a RandomForestClassifier and an SVC ...\n",
    "\n",
    "The example is shown in [the accompanying notebook](https://github.com/amueller/introduction_to_ml_with_python/blob/master/06-algorithm-chains-and-pipelines.ipynb), code cells 35-37. Could you apply this concept to your own pipelines?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Categorical Encodings\n",
    "\n",
    "**1.** The article **[Categorical Features and Encoding in Decision Trees](https://medium.com/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931)** mentions 4 encodings:\n",
    "\n",
    "- **\"Categorical Encoding\":** This means using the raw categorical values as-is, not encoded. Scikit-learn doesn't support this, but some tree algorithm implementations do. For example, [Catboost](https://catboost.ai/), or R's [rpart](https://cran.r-project.org/web/packages/rpart/index.html) package.\n",
    "- **Numeric Encoding:** Synonymous with Label Encoding, or \"Ordinal\" Encoding with random order. We can use [category_encoders.OrdinalEncoder](https://contrib.scikit-learn.org/categorical-encoding/ordinal.html).\n",
    "- **One-Hot Encoding:** We can use [category_encoders.OneHotEncoder](http://contrib.scikit-learn.org/categorical-encoding/onehot.html).\n",
    "- **Binary Encoding:** We can use [category_encoders.BinaryEncoder](http://contrib.scikit-learn.org/categorical-encoding/binary.html).\n",
    "\n",
    "\n",
    "**2.** The short video \n",
    "**[Coursera — How to Win a Data Science Competition: Learn from Top Kagglers — Concept of mean encoding](https://www.coursera.org/lecture/competitive-data-science/concept-of-mean-encoding-b5Gxv)** introduces an interesting idea: use both X _and_ y to encode categoricals.\n",
    "\n",
    "Category Encoders has multiple implementations of this general concept:\n",
    "\n",
    "- [CatBoost Encoder](http://contrib.scikit-learn.org/categorical-encoding/catboost.html)\n",
    "- [James-Stein Encoder](http://contrib.scikit-learn.org/categorical-encoding/jamesstein.html)\n",
    "- [Leave One Out](http://contrib.scikit-learn.org/categorical-encoding/leaveoneout.html)\n",
    "- [M-estimate](http://contrib.scikit-learn.org/categorical-encoding/mestimate.html)\n",
    "- [Target Encoder](http://contrib.scikit-learn.org/categorical-encoding/targetencoder.html)\n",
    "- [Weight of Evidence](http://contrib.scikit-learn.org/categorical-encoding/woe.html)\n",
    "\n",
    "Category Encoder's mean encoding implementations work for regression problems or binary classification problems. \n",
    "\n",
    "For multi-class classification problems, you will need to temporarily reformulate it as binary classification. For example:\n",
    "\n",
    "```python\n",
    "encoder = ce.TargetEncoder(min_samples_leaf=..., smoothing=...) # Both parameters > 1 to avoid overfitting\n",
    "X_train_encoded = encoder.fit_transform(X_train, y_train=='functional')\n",
    "X_val_encoded = encoder.transform(X_train, y_val=='functional')\n",
    "```\n",
    "\n",
    "**3.** The **[dirty_cat](https://dirty-cat.github.io/stable/)** library has a Target Encoder implementation that works with multi-class classification.\n",
    "\n",
    "```python\n",
    " dirty_cat.TargetEncoder(clf_type='multiclass-clf')\n",
    "```\n",
    "It also implements an interesting idea called [\"Similarity Encoder\" for dirty categories](https://www.slideshare.net/GaelVaroquaux/machine-learning-on-non-curated-data-154905090).\n",
    "\n",
    "However, it seems like dirty_cat doesn't handle missing values or unknown categories as well as category_encoders does. And you may need to use it with one column at a time, instead of with your whole dataframe.\n",
    "\n",
    "**4. [Embeddings](https://www.kaggle.com/learn/embeddings)** can work well with sparse / high cardinality categoricals.\n",
    "\n",
    "_**I hope it’s not too frustrating or confusing that there’s not one “canonical” way to encode categorcals. It’s an active area of research and experimentation! Maybe you can make your own contributions!**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS: Stacking!\n",
    "\n",
    "Here's some code you can use to \"stack\" multiple submissions, which is another form of ensembling:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Filenames of your submissions you want to ensemble\n",
    "files = ['submission-01.csv', 'submission-02.csv', 'submission-03.csv']\n",
    "\n",
    "target = 'status_group'\n",
    "submissions = (pd.read_csv(file)[[target]] for file in files)\n",
    "ensemble = pd.concat(submissions, axis='columns')\n",
    "majority_vote = ensemble.mode(axis='columns')[0]\n",
    "\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "submission = sample_submission.copy()\n",
    "submission[target] = majority_vote\n",
    "submission.to_csv('my-ultimate-ensemble-submission.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "in_colab = 'google.colab' in sys.modules\n",
    "\n",
    "# If you're in Colab...\n",
    "if in_colab:\n",
    "    # Pull files from Github repo\n",
    "    os.chdir('/content')\n",
    "    !git init .\n",
    "    !git remote add origin https://github.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge.git\n",
    "    !git pull origin master\n",
    "    \n",
    "    # Install required python packages\n",
    "    !pip install -r requirements.txt\n",
    "    \n",
    "    # Change into directory for module\n",
    "    os.chdir('module3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Merge train_features.csv & train_labels.csv\n",
    "train = pd.merge(pd.read_csv('../data/tanzania/train_features.csv'), \n",
    "                 pd.read_csv('../data/tanzania/train_labels.csv'))\n",
    "\n",
    "# Read test_features.csv & sample_submission.csv\n",
    "test = pd.read_csv('../data/tanzania/test_features.csv')\n",
    "sample_submission = pd.read_csv('../data/tanzania/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, validate = train_test_split(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import f_regression, SelectKBest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "\n",
    "def getXandY(df):\n",
    "  X=df.drop(columns=['id','status_group']).copy()\n",
    "  X.date_recorded=pd.to_datetime(X.date_recorded, infer_datetime_format=True)\n",
    "  X.latitude=X.latitude.replace(-2e-08,0)\n",
    "  cols_with_zeros=['longitude','latitude','construction_year','population']\n",
    "  for col in cols_with_zeros:\n",
    "    X[col]=X[col].replace(0,np.nan)\n",
    "    X[col+'_MISSING']=X[col].copy().isnull()\n",
    "  X=X.drop(columns=['quantity_group',\"payment_type\"])\n",
    "  X=X.drop(columns=\"recorded_by\")\n",
    "  X['year_recorded'] = X['date_recorded'].dt.year\n",
    "  X['month_recorded'] = X['date_recorded'].dt.month\n",
    "  X['day_recorded'] = X['date_recorded'].dt.day\n",
    "  \n",
    "  X['Yrs_before_inspection']=X.date_recorded.dt.year-X.construction_year\n",
    "  X['yrs_before_inspection_MISSING']=X.Yrs_before_inspection.isnull()\n",
    "  X = X.drop(columns='date_recorded')\n",
    "  return X, df.status_group.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getX(df):\n",
    "  X=df.drop(columns=['id']).copy()\n",
    "  X.date_recorded=pd.to_datetime(X.date_recorded, infer_datetime_format=True)\n",
    "  X.latitude=X.latitude.replace(-2e-08,0)\n",
    "  cols_with_zeros=['longitude','latitude','construction_year','population']\n",
    "  for col in cols_with_zeros:\n",
    "    X[col]=X[col].replace(0,np.nan)\n",
    "    X[col+'_MISSING']=X[col].copy().isnull()\n",
    "  X=X.drop(columns=['quantity_group',\"payment_type\"])\n",
    "  X=X.drop(columns=\"recorded_by\")\n",
    "  X['year_recorded'] = X['date_recorded'].dt.year\n",
    "  X['month_recorded'] = X['date_recorded'].dt.month\n",
    "  X['day_recorded'] = X['date_recorded'].dt.day\n",
    "  \n",
    "  X['Yrs_before_inspection']=X.date_recorded.dt.year-X.construction_year\n",
    "  X['yrs_before_inspection_MISSING']=X.Yrs_before_inspection.isnull()\n",
    "  X = X.drop(columns='date_recorded')\n",
    "  return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = getXandY(train)\n",
    "val_x, val_y=getXandY(validate)\n",
    "test_x=getX(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numeric_target=train_y.replace({'non functional':0,\n",
    "                              'functional needs repair':1,\n",
    "                              'functional':2})\n",
    "# train_x['numeric_target']=numeric_target\n",
    "# target='numeric_target'\n",
    "\n",
    "pipeline=make_pipeline(\n",
    "\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(),\n",
    "    StandardScaler(),\n",
    "    SelectKBest(f_regression),\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "scores=cross_val_score(pipeline, train_x, numeric_target, cv=5, scoring='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 5 folds: [0.7567052  0.72382449 0.73459769 0.73487485 0.73484508]\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy for 5 folds: {scores}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The coverage_error function computes the average number of labels that have to be included in the final prediction such that all true labels are predicted. This is useful if you want to know how many top-scored-labels you have to predict in average without missing any true one. The best value of this metrics is thus the average number of true labels.\n",
    "\n",
    "Note\n",
    "\n",
    "Our implementation’s score is 1 greater than the one given in Tsoumakas et al., 2010. This extends it to handle the degenerate case in which an instance has 0 true labels.\n",
    "\n",
    "Formally, given a binary indicator matrix of the ground truth labels\n",
    "and the score associated with each label\n",
    "\n",
    ", the coverage is defined as\n",
    "\n",
    "with\n",
    "\n",
    ". Given the rank definition, ties in y_scores are broken by giving the maximal rank that would have been assigned to all tied values.\n",
    "\n",
    "Here is a small example of usage of this function:\n",
    ">>>\n",
    "\n",
    ">>> import numpy as np\n",
    ">>> from sklearn.metrics import coverage_error\n",
    ">>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
    ">>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
    ">>> coverage_error(y_true, y_score)\n",
    "2.5\n",
    "\n",
    "3.3.3.2. Label ranking average precision\n",
    "\n",
    "The label_ranking_average_precision_score function implements label ranking average precision (LRAP). This metric is linked to the average_precision_score function, but is based on the notion of label ranking instead of precision and recall.\n",
    "\n",
    "Label ranking average precision (LRAP) averages over the samples the answer to the following question: for each ground truth label, what fraction of higher-ranked labels were true labels? This performance measure will be higher if you are able to give better rank to the labels associated with each sample. The obtained score is always strictly greater than 0, and the best value is 1. If there is exactly one relevant label per sample, label ranking average precision is equivalent to the mean reciprocal rank.\n",
    "\n",
    "Formally, given a binary indicator matrix of the ground truth labels\n",
    "and the score associated with each label\n",
    "\n",
    ", the average precision is defined as\n",
    "\n",
    "where\n",
    ", , computes the cardinality of the set (i.e., the number of elements in the set), and is the “norm” (which\n",
    "\n",
    "The coverage_error function computes the average number of labels that have to be included in the final prediction such that all true labels are predicted. This is useful if you want to know how many top-scored-labels you have to predict in average without missing any true one. The best value of this metrics is thus the average number of true labels.\n",
    "\n",
    "Note\n",
    "\n",
    "Our implementation’s score is 1 greater than the one given in Tsoumakas et al., 2010. This extends it to handle the degenerate case in which an instance has 0 true labels.\n",
    "\n",
    "Formally, given a binary indicator matrix of the ground truth labels\n",
    "and the score associated with each label\n",
    "\n",
    ", the coverage is defined as\n",
    "\n",
    "with\n",
    "\n",
    ". Given the rank definition, ties in y_scores are broken by giving the maximal rank that would have been assigned to all tied values.\n",
    "\n",
    "Here is a small example of usage of this function:\n",
    ">>>\n",
    "\n",
    ">>> import numpy as np\n",
    ">>> from sklearn.metrics import coverage_error\n",
    ">>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
    ">>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
    ">>> coverage_error(y_true, y_score)\n",
    "2.5\n",
    "\n",
    "3.3.3.2. Label ranking average precision\n",
    "\n",
    "The label_ranking_average_precision_score function implements label ranking average precision (LRAP). This metric is linked to the average_precision_score function, but is based on the notion of label ranking instead of precision and recall.\n",
    "\n",
    "Label ranking average precision (LRAP) averages over the samples the answer to the following question: for each ground truth label, what fraction of higher-ranked labels were true labels? This performance measure will be higher if you are able to give better rank to the labels associated with each sample. The obtained score is always strictly greater than 0, and the best value is 1. If there is exactly one relevant label per sample, label ranking average precision is equivalent to the mean reciprocal rank.\n",
    "\n",
    "Formally, given a binary indicator matrix of the ground truth labels\n",
    "and the score associated with each label\n",
    "\n",
    ", the average precision is defined as\n",
    "\n",
    "where\n",
    ", , computes the cardinality of the set (i.e., the number of elements in the set), and is the “norm” (which\n",
    "\n",
    "The coverage_error function computes the average number of labels that have to be included in the final prediction such that all true labels are predicted. This is useful if you want to know how many top-scored-labels you have to predict in average without missing any true one. The best value of this metrics is thus the average number of true labels.\n",
    "\n",
    "Note\n",
    "\n",
    "Our implementation’s score is 1 greater than the one given in Tsoumakas et al., 2010. This extends it to handle the degenerate case in which an instance has 0 true labels.\n",
    "\n",
    "Formally, given a binary indicator matrix of the ground truth labels\n",
    "and the score associated with each label\n",
    "\n",
    ", the coverage is defined as\n",
    "\n",
    "with\n",
    "\n",
    ". Given the rank definition, ties in y_scores are broken by giving the maximal rank that would have been assigned to all tied values.\n",
    "\n",
    "Here is a small example of usage of this function:\n",
    ">>>\n",
    "\n",
    ">>> import numpy as np\n",
    ">>> from sklearn.metrics import coverage_error\n",
    ">>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
    ">>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
    ">>> coverage_error(y_true, y_score)\n",
    "2.5\n",
    "\n",
    "3.3.3.2. Label ranking average precision\n",
    "\n",
    "The label_ranking_average_precision_score function implements label ranking average precision (LRAP). This metric is linked to the average_precision_score function, but is based on the notion of label ranking instead of precision and recall.\n",
    "\n",
    "Label ranking average precision (LRAP) averages over the samples the answer to the following question: for each ground truth label, what fraction of higher-ranked labels were true labels? This performance measure will be higher if you are able to give better rank to the labels associated with each sample. The obtained score is always strictly greater than 0, and the best value is 1. If there is exactly one relevant label per sample, label ranking average precision is equivalent to the mean reciprocal rank.\n",
    "\n",
    "Formally, given a binary indicator matrix of the ground truth labels\n",
    "and the score associated with each label\n",
    "\n",
    ", the average precision is defined as\n",
    "\n",
    "where\n",
    ", , computes the cardinality of the set (i.e., the number of elements in the set), and is the “norm” (which"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "pipeline=make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(),\n",
    "    SelectKBest(),\n",
    "    ExtraTreesClassifier()\n",
    "\n",
    ") \n",
    "depth=range(2,30,3) \n",
    "param_distributions={\n",
    "        'simpleimputer__strategy':['mean','median'],\n",
    "        'selectkbest__k': range(1, len(train_x.columns)+1),\n",
    "        'extratreesclassifier__max_depth':depth,\n",
    "        'extratreesclassifier__max_features': uniform(0,1)\n",
    "}\n",
    "\n",
    "train_scores, val_scores = validation_curve(\n",
    "    pipeline, train_x, train_y,\n",
    "    param_name='extratreesclassifier__max_depth',\n",
    "    param_range=range(2,30,3), scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    "  \n",
    "\n",
    ")\n",
    "plt.plot(depth, np.mean(train_scores, axis=1), color='blue', label='training error')\n",
    "plt.plot(depth, np.mean(val_scores, axis=1), color='red', label='validation error')\n",
    "plt.title('Validation Curve')\n",
    "plt.xlabel('model complexity: ExtraTreesClassifier max_depth')\n",
    "plt.ylabel('model score: Accuracy')\n",
    "\n",
    "plt.legend(); #17 or 18 is our best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   35.7s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:   51.3s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed: 12.8min\n",
      "[Parallel(n_jobs=-1)]: Done 238 tasks      | elapsed: 14.1min\n",
      "[Parallel(n_jobs=-1)]: Done 261 tasks      | elapsed: 15.5min\n",
      "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed: 17.0min\n",
      "[Parallel(n_jobs=-1)]: Done 309 tasks      | elapsed: 18.4min\n",
      "[Parallel(n_jobs=-1)]: Done 334 tasks      | elapsed: 19.9min\n",
      "[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed: 21.5min\n",
      "[Parallel(n_jobs=-1)]: Done 388 tasks      | elapsed: 23.2min\n",
      "[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed: 24.9min\n",
      "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed: 26.6min\n",
      "[Parallel(n_jobs=-1)]: Done 477 tasks      | elapsed: 28.4min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 29.7min finished\n",
      "/home/jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter extratreesclassifier for estimator Pipeline(memory=None,\n     steps=[('ordinalencoder', OrdinalEncoder(cols=None, drop_invariant=False, handle_unknown='impute',\n        impute_missing=True, mapping=None, return_df=True, verbose=0)), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy='mean',\n       verbose=0)), ('selectkbes...obs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False))]). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/jesse/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"/home/jesse/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/home/jesse/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/jesse/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"/home/jesse/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/home/jesse/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 514, in _fit_and_score\n    estimator.set_params(**parameters)\n  File \"/home/jesse/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\", line 147, in set_params\n    self._set_params('steps', **kwargs)\n  File \"/home/jesse/anaconda3/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\", line 52, in _set_params\n    super(_BaseComposition, self).set_params(**params)\n  File \"/home/jesse/anaconda3/lib/python3.7/site-packages/sklearn/base.py\", line 215, in set_params\n    (key, self))\nValueError: Invalid parameter extratreesclassifier for estimator Pipeline(memory=None,\n     steps=[('ordinalencoder', OrdinalEncoder(cols=None, drop_invariant=False, handle_unknown='impute',\n        impute_missing=True, mapping=None, return_df=True, verbose=0)), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy='mean',\n       verbose=0)), ('selectkbes...obs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False))]). Check the list of available parameters with `estimator.get_params().keys()`.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-84eed09b7db8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mparam_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mvalidation_curve\u001b[0;34m(estimator, X, y, param_name, param_range, groups, cv, scoring, n_jobs, pre_dispatch, verbose, error_score)\u001b[0m\n\u001b[1;32m   1445\u001b[0m         error_score=error_score)\n\u001b[1;32m   1446\u001b[0m         \u001b[0;31m# NOTE do not change order of iteration to allow one time cv splitters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1447\u001b[0;31m         for train, test in cv.split(X, y, groups) for v in param_range)\n\u001b[0m\u001b[1;32m   1448\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m     \u001b[0mn_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter extratreesclassifier for estimator Pipeline(memory=None,\n     steps=[('ordinalencoder', OrdinalEncoder(cols=None, drop_invariant=False, handle_unknown='impute',\n        impute_missing=True, mapping=None, return_df=True, verbose=0)), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy='mean',\n       verbose=0)), ('selectkbes...obs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False))]). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "pipeline=make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(),\n",
    "    SelectKBest(),\n",
    "    RandomForestClassifier()\n",
    "\n",
    ") \n",
    "depth=range(10,30,5) \n",
    "param_distributions={\n",
    "        'simpleimputer__strategy':['mean','median'],\n",
    "        'selectkbest__k': range(30, len(train_x.columns)),\n",
    "        'randomforestclassifier__max_depth':depth,\n",
    "        \n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_distributions, \n",
    "    n_iter=100, \n",
    "    cv=5, \n",
    "    scoring='accuracy', \n",
    "    verbose=10, \n",
    "    return_train_score=True, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(train_x, train_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperperameters {'simpleimputer__strategy': 'mean', 'selectkbest__k': 34, 'randomforestclassifier__max_depth': 20}\n",
      "Cross validation accuracy 0.7914478114478114\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Hyperperameters\", search.best_params_)\n",
    "print(\"Cross validation accuracy\", search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Hyperperameters {'randomforestclassifier__max_depth': 20, 'randomforestclassifier__max_features': 0.2697913598431557, 'selectkbest__k': 33, 'simpleimputer__strategy': 'mean'}\n",
    "Cross validation accuracy 0.7892255892255893"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_simpleimputer__strategy</th>\n",
       "      <th>param_selectkbest__k</th>\n",
       "      <th>param_randomforestclassifier__max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.619744</td>\n",
       "      <td>0.182049</td>\n",
       "      <td>0.493774</td>\n",
       "      <td>0.015340</td>\n",
       "      <td>mean</td>\n",
       "      <td>38</td>\n",
       "      <td>21</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.777915</td>\n",
       "      <td>0.791943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784691</td>\n",
       "      <td>0.004445</td>\n",
       "      <td>78</td>\n",
       "      <td>0.949325</td>\n",
       "      <td>0.951289</td>\n",
       "      <td>0.957518</td>\n",
       "      <td>0.950226</td>\n",
       "      <td>0.956680</td>\n",
       "      <td>0.953008</td>\n",
       "      <td>0.003408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.536158</td>\n",
       "      <td>0.104687</td>\n",
       "      <td>0.529449</td>\n",
       "      <td>0.015119</td>\n",
       "      <td>median</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.781955</td>\n",
       "      <td>0.794299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786914</td>\n",
       "      <td>0.004039</td>\n",
       "      <td>43</td>\n",
       "      <td>0.938747</td>\n",
       "      <td>0.948483</td>\n",
       "      <td>0.947445</td>\n",
       "      <td>0.950478</td>\n",
       "      <td>0.948965</td>\n",
       "      <td>0.946824</td>\n",
       "      <td>0.004155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.512257</td>\n",
       "      <td>0.178875</td>\n",
       "      <td>0.532015</td>\n",
       "      <td>0.009846</td>\n",
       "      <td>median</td>\n",
       "      <td>43</td>\n",
       "      <td>18</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.784424</td>\n",
       "      <td>0.788239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786779</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>46</td>\n",
       "      <td>0.917562</td>\n",
       "      <td>0.927664</td>\n",
       "      <td>0.921126</td>\n",
       "      <td>0.918661</td>\n",
       "      <td>0.920291</td>\n",
       "      <td>0.921061</td>\n",
       "      <td>0.003527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.413463</td>\n",
       "      <td>0.040377</td>\n",
       "      <td>0.541752</td>\n",
       "      <td>0.019198</td>\n",
       "      <td>median</td>\n",
       "      <td>39</td>\n",
       "      <td>21</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.787566</td>\n",
       "      <td>0.784873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785791</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>66</td>\n",
       "      <td>0.949830</td>\n",
       "      <td>0.953955</td>\n",
       "      <td>0.957687</td>\n",
       "      <td>0.956090</td>\n",
       "      <td>0.956877</td>\n",
       "      <td>0.954888</td>\n",
       "      <td>0.002817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.407448</td>\n",
       "      <td>0.122330</td>\n",
       "      <td>0.543203</td>\n",
       "      <td>0.016223</td>\n",
       "      <td>median</td>\n",
       "      <td>38</td>\n",
       "      <td>23</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.777915</td>\n",
       "      <td>0.784648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.781235</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>100</td>\n",
       "      <td>0.962429</td>\n",
       "      <td>0.968181</td>\n",
       "      <td>0.970173</td>\n",
       "      <td>0.965545</td>\n",
       "      <td>0.968492</td>\n",
       "      <td>0.966964</td>\n",
       "      <td>0.002709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.807020</td>\n",
       "      <td>0.136344</td>\n",
       "      <td>0.551015</td>\n",
       "      <td>0.028082</td>\n",
       "      <td>mean</td>\n",
       "      <td>32</td>\n",
       "      <td>19</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.788464</td>\n",
       "      <td>0.793850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790101</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>6</td>\n",
       "      <td>0.915654</td>\n",
       "      <td>0.923511</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.931540</td>\n",
       "      <td>0.924976</td>\n",
       "      <td>0.924736</td>\n",
       "      <td>0.005310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.276196</td>\n",
       "      <td>0.076805</td>\n",
       "      <td>0.534697</td>\n",
       "      <td>0.009830</td>\n",
       "      <td>mean</td>\n",
       "      <td>42</td>\n",
       "      <td>24</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.780720</td>\n",
       "      <td>0.783863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.782918</td>\n",
       "      <td>0.003613</td>\n",
       "      <td>95</td>\n",
       "      <td>0.970257</td>\n",
       "      <td>0.975813</td>\n",
       "      <td>0.977553</td>\n",
       "      <td>0.975562</td>\n",
       "      <td>0.975983</td>\n",
       "      <td>0.975034</td>\n",
       "      <td>0.002488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.026540</td>\n",
       "      <td>0.093005</td>\n",
       "      <td>0.536502</td>\n",
       "      <td>0.010497</td>\n",
       "      <td>median</td>\n",
       "      <td>31</td>\n",
       "      <td>19</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.787790</td>\n",
       "      <td>0.791718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.788799</td>\n",
       "      <td>0.002206</td>\n",
       "      <td>12</td>\n",
       "      <td>0.911417</td>\n",
       "      <td>0.923651</td>\n",
       "      <td>0.927579</td>\n",
       "      <td>0.928229</td>\n",
       "      <td>0.922591</td>\n",
       "      <td>0.922694</td>\n",
       "      <td>0.006043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.535480</td>\n",
       "      <td>0.072411</td>\n",
       "      <td>0.537862</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>median</td>\n",
       "      <td>42</td>\n",
       "      <td>20</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.779374</td>\n",
       "      <td>0.790933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787452</td>\n",
       "      <td>0.004221</td>\n",
       "      <td>34</td>\n",
       "      <td>0.945032</td>\n",
       "      <td>0.943573</td>\n",
       "      <td>0.950728</td>\n",
       "      <td>0.944979</td>\n",
       "      <td>0.949891</td>\n",
       "      <td>0.946841</td>\n",
       "      <td>0.002892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.136824</td>\n",
       "      <td>0.121963</td>\n",
       "      <td>0.541445</td>\n",
       "      <td>0.005696</td>\n",
       "      <td>mean</td>\n",
       "      <td>39</td>\n",
       "      <td>21</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.779374</td>\n",
       "      <td>0.782179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784489</td>\n",
       "      <td>0.003415</td>\n",
       "      <td>79</td>\n",
       "      <td>0.940935</td>\n",
       "      <td>0.954713</td>\n",
       "      <td>0.956312</td>\n",
       "      <td>0.957016</td>\n",
       "      <td>0.956175</td>\n",
       "      <td>0.953030</td>\n",
       "      <td>0.006094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.837650</td>\n",
       "      <td>0.047664</td>\n",
       "      <td>0.548320</td>\n",
       "      <td>0.017399</td>\n",
       "      <td>mean</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.783526</td>\n",
       "      <td>0.787341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787003</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>41</td>\n",
       "      <td>0.956087</td>\n",
       "      <td>0.958921</td>\n",
       "      <td>0.963158</td>\n",
       "      <td>0.962655</td>\n",
       "      <td>0.964648</td>\n",
       "      <td>0.961094</td>\n",
       "      <td>0.003134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.896381</td>\n",
       "      <td>0.111690</td>\n",
       "      <td>0.545593</td>\n",
       "      <td>0.012163</td>\n",
       "      <td>mean</td>\n",
       "      <td>35</td>\n",
       "      <td>24</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.783077</td>\n",
       "      <td>0.781618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.782020</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>99</td>\n",
       "      <td>0.968798</td>\n",
       "      <td>0.974550</td>\n",
       "      <td>0.972839</td>\n",
       "      <td>0.970399</td>\n",
       "      <td>0.967763</td>\n",
       "      <td>0.970870</td>\n",
       "      <td>0.002514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.756494</td>\n",
       "      <td>0.083364</td>\n",
       "      <td>0.538632</td>\n",
       "      <td>0.007623</td>\n",
       "      <td>mean</td>\n",
       "      <td>30</td>\n",
       "      <td>18</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.788239</td>\n",
       "      <td>0.793289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789764</td>\n",
       "      <td>0.001827</td>\n",
       "      <td>7</td>\n",
       "      <td>0.898566</td>\n",
       "      <td>0.908527</td>\n",
       "      <td>0.912540</td>\n",
       "      <td>0.914284</td>\n",
       "      <td>0.906515</td>\n",
       "      <td>0.908086</td>\n",
       "      <td>0.005505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.270776</td>\n",
       "      <td>0.098646</td>\n",
       "      <td>0.547203</td>\n",
       "      <td>0.007464</td>\n",
       "      <td>mean</td>\n",
       "      <td>42</td>\n",
       "      <td>20</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.783638</td>\n",
       "      <td>0.788688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786734</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>48</td>\n",
       "      <td>0.937765</td>\n",
       "      <td>0.947417</td>\n",
       "      <td>0.952159</td>\n",
       "      <td>0.946438</td>\n",
       "      <td>0.947506</td>\n",
       "      <td>0.946257</td>\n",
       "      <td>0.004688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.167279</td>\n",
       "      <td>0.413744</td>\n",
       "      <td>0.540980</td>\n",
       "      <td>0.007797</td>\n",
       "      <td>mean</td>\n",
       "      <td>34</td>\n",
       "      <td>20</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.791830</td>\n",
       "      <td>0.795309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.791448</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>1</td>\n",
       "      <td>0.927692</td>\n",
       "      <td>0.937540</td>\n",
       "      <td>0.941637</td>\n",
       "      <td>0.938919</td>\n",
       "      <td>0.941025</td>\n",
       "      <td>0.937362</td>\n",
       "      <td>0.005053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.079068</td>\n",
       "      <td>0.097265</td>\n",
       "      <td>0.537157</td>\n",
       "      <td>0.007789</td>\n",
       "      <td>mean</td>\n",
       "      <td>39</td>\n",
       "      <td>19</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.786107</td>\n",
       "      <td>0.787903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787475</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>33</td>\n",
       "      <td>0.924661</td>\n",
       "      <td>0.932995</td>\n",
       "      <td>0.939112</td>\n",
       "      <td>0.935832</td>\n",
       "      <td>0.936227</td>\n",
       "      <td>0.933765</td>\n",
       "      <td>0.004948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.554545</td>\n",
       "      <td>0.081526</td>\n",
       "      <td>0.546218</td>\n",
       "      <td>0.009657</td>\n",
       "      <td>median</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.781169</td>\n",
       "      <td>0.793289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787520</td>\n",
       "      <td>0.003871</td>\n",
       "      <td>30</td>\n",
       "      <td>0.970005</td>\n",
       "      <td>0.974494</td>\n",
       "      <td>0.974382</td>\n",
       "      <td>0.975253</td>\n",
       "      <td>0.976460</td>\n",
       "      <td>0.974119</td>\n",
       "      <td>0.002187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.145304</td>\n",
       "      <td>0.051479</td>\n",
       "      <td>0.550549</td>\n",
       "      <td>0.014929</td>\n",
       "      <td>median</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.786444</td>\n",
       "      <td>0.783975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785948</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>65</td>\n",
       "      <td>0.939364</td>\n",
       "      <td>0.937400</td>\n",
       "      <td>0.949662</td>\n",
       "      <td>0.946943</td>\n",
       "      <td>0.947590</td>\n",
       "      <td>0.944192</td>\n",
       "      <td>0.004868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.843284</td>\n",
       "      <td>0.072202</td>\n",
       "      <td>0.544886</td>\n",
       "      <td>0.010765</td>\n",
       "      <td>mean</td>\n",
       "      <td>33</td>\n",
       "      <td>23</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.790259</td>\n",
       "      <td>0.786668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>49</td>\n",
       "      <td>0.956677</td>\n",
       "      <td>0.964056</td>\n",
       "      <td>0.963383</td>\n",
       "      <td>0.963918</td>\n",
       "      <td>0.962292</td>\n",
       "      <td>0.962065</td>\n",
       "      <td>0.002765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.789494</td>\n",
       "      <td>0.054912</td>\n",
       "      <td>0.544174</td>\n",
       "      <td>0.007697</td>\n",
       "      <td>mean</td>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.786444</td>\n",
       "      <td>0.781394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786958</td>\n",
       "      <td>0.004085</td>\n",
       "      <td>42</td>\n",
       "      <td>0.949578</td>\n",
       "      <td>0.958052</td>\n",
       "      <td>0.951879</td>\n",
       "      <td>0.962347</td>\n",
       "      <td>0.958925</td>\n",
       "      <td>0.956156</td>\n",
       "      <td>0.004715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.054221</td>\n",
       "      <td>0.054029</td>\n",
       "      <td>0.599293</td>\n",
       "      <td>0.069121</td>\n",
       "      <td>mean</td>\n",
       "      <td>38</td>\n",
       "      <td>19</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.783302</td>\n",
       "      <td>0.786107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785140</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>71</td>\n",
       "      <td>0.922893</td>\n",
       "      <td>0.924409</td>\n",
       "      <td>0.936755</td>\n",
       "      <td>0.928229</td>\n",
       "      <td>0.934880</td>\n",
       "      <td>0.929433</td>\n",
       "      <td>0.005527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.406868</td>\n",
       "      <td>0.191238</td>\n",
       "      <td>0.547975</td>\n",
       "      <td>0.011324</td>\n",
       "      <td>median</td>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.779374</td>\n",
       "      <td>0.783189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783322</td>\n",
       "      <td>0.002363</td>\n",
       "      <td>90</td>\n",
       "      <td>0.967115</td>\n",
       "      <td>0.968798</td>\n",
       "      <td>0.974803</td>\n",
       "      <td>0.967594</td>\n",
       "      <td>0.968296</td>\n",
       "      <td>0.969321</td>\n",
       "      <td>0.002801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.141467</td>\n",
       "      <td>0.115484</td>\n",
       "      <td>0.544680</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>median</td>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.785995</td>\n",
       "      <td>0.788800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787946</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>20</td>\n",
       "      <td>0.924437</td>\n",
       "      <td>0.930946</td>\n",
       "      <td>0.937232</td>\n",
       "      <td>0.933896</td>\n",
       "      <td>0.933505</td>\n",
       "      <td>0.932003</td>\n",
       "      <td>0.004279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.298831</td>\n",
       "      <td>0.236699</td>\n",
       "      <td>0.578854</td>\n",
       "      <td>0.041553</td>\n",
       "      <td>mean</td>\n",
       "      <td>36</td>\n",
       "      <td>22</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.781730</td>\n",
       "      <td>0.785097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784848</td>\n",
       "      <td>0.002972</td>\n",
       "      <td>75</td>\n",
       "      <td>0.958360</td>\n",
       "      <td>0.961419</td>\n",
       "      <td>0.962008</td>\n",
       "      <td>0.955136</td>\n",
       "      <td>0.955698</td>\n",
       "      <td>0.958524</td>\n",
       "      <td>0.002829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.965623</td>\n",
       "      <td>0.035553</td>\n",
       "      <td>0.554278</td>\n",
       "      <td>0.006278</td>\n",
       "      <td>mean</td>\n",
       "      <td>35</td>\n",
       "      <td>18</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.781618</td>\n",
       "      <td>0.787117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786824</td>\n",
       "      <td>0.003011</td>\n",
       "      <td>45</td>\n",
       "      <td>0.898398</td>\n",
       "      <td>0.914560</td>\n",
       "      <td>0.922893</td>\n",
       "      <td>0.913302</td>\n",
       "      <td>0.916166</td>\n",
       "      <td>0.913064</td>\n",
       "      <td>0.008045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.449733</td>\n",
       "      <td>0.783896</td>\n",
       "      <td>0.547931</td>\n",
       "      <td>0.012573</td>\n",
       "      <td>median</td>\n",
       "      <td>42</td>\n",
       "      <td>21</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.783077</td>\n",
       "      <td>0.785209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786083</td>\n",
       "      <td>0.001975</td>\n",
       "      <td>62</td>\n",
       "      <td>0.949914</td>\n",
       "      <td>0.963046</td>\n",
       "      <td>0.959651</td>\n",
       "      <td>0.959036</td>\n",
       "      <td>0.959907</td>\n",
       "      <td>0.958311</td>\n",
       "      <td>0.004422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4.286354</td>\n",
       "      <td>0.199870</td>\n",
       "      <td>0.558261</td>\n",
       "      <td>0.023635</td>\n",
       "      <td>median</td>\n",
       "      <td>35</td>\n",
       "      <td>19</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.784199</td>\n",
       "      <td>0.788239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787924</td>\n",
       "      <td>0.003103</td>\n",
       "      <td>21</td>\n",
       "      <td>0.922865</td>\n",
       "      <td>0.929796</td>\n",
       "      <td>0.929010</td>\n",
       "      <td>0.932325</td>\n",
       "      <td>0.922535</td>\n",
       "      <td>0.927306</td>\n",
       "      <td>0.003919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.227367</td>\n",
       "      <td>0.082260</td>\n",
       "      <td>0.565833</td>\n",
       "      <td>0.016955</td>\n",
       "      <td>median</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.786556</td>\n",
       "      <td>0.782404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784759</td>\n",
       "      <td>0.003320</td>\n",
       "      <td>77</td>\n",
       "      <td>0.963804</td>\n",
       "      <td>0.962345</td>\n",
       "      <td>0.966161</td>\n",
       "      <td>0.965405</td>\n",
       "      <td>0.963386</td>\n",
       "      <td>0.964220</td>\n",
       "      <td>0.001382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.231173</td>\n",
       "      <td>0.052836</td>\n",
       "      <td>0.578269</td>\n",
       "      <td>0.034020</td>\n",
       "      <td>mean</td>\n",
       "      <td>40</td>\n",
       "      <td>19</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.787117</td>\n",
       "      <td>0.788015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787520</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>30</td>\n",
       "      <td>0.920396</td>\n",
       "      <td>0.933584</td>\n",
       "      <td>0.933444</td>\n",
       "      <td>0.932045</td>\n",
       "      <td>0.935217</td>\n",
       "      <td>0.930937</td>\n",
       "      <td>0.005365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.440418</td>\n",
       "      <td>0.065937</td>\n",
       "      <td>0.590479</td>\n",
       "      <td>0.020410</td>\n",
       "      <td>median</td>\n",
       "      <td>37</td>\n",
       "      <td>23</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.783302</td>\n",
       "      <td>0.782404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783120</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>93</td>\n",
       "      <td>0.959483</td>\n",
       "      <td>0.970005</td>\n",
       "      <td>0.967844</td>\n",
       "      <td>0.961870</td>\n",
       "      <td>0.966556</td>\n",
       "      <td>0.965152</td>\n",
       "      <td>0.003891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>4.030811</td>\n",
       "      <td>0.366559</td>\n",
       "      <td>0.590226</td>\n",
       "      <td>0.076757</td>\n",
       "      <td>mean</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.788239</td>\n",
       "      <td>0.793514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790168</td>\n",
       "      <td>0.001879</td>\n",
       "      <td>4</td>\n",
       "      <td>0.905693</td>\n",
       "      <td>0.908752</td>\n",
       "      <td>0.916496</td>\n",
       "      <td>0.908140</td>\n",
       "      <td>0.916250</td>\n",
       "      <td>0.911066</td>\n",
       "      <td>0.004453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>4.256714</td>\n",
       "      <td>0.124252</td>\n",
       "      <td>0.597869</td>\n",
       "      <td>0.059610</td>\n",
       "      <td>mean</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.790708</td>\n",
       "      <td>0.788351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789675</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>8</td>\n",
       "      <td>0.919807</td>\n",
       "      <td>0.916720</td>\n",
       "      <td>0.933219</td>\n",
       "      <td>0.926124</td>\n",
       "      <td>0.921525</td>\n",
       "      <td>0.923479</td>\n",
       "      <td>0.005742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>4.780695</td>\n",
       "      <td>0.381782</td>\n",
       "      <td>0.623917</td>\n",
       "      <td>0.047306</td>\n",
       "      <td>median</td>\n",
       "      <td>34</td>\n",
       "      <td>20</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.788464</td>\n",
       "      <td>0.790035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.002240</td>\n",
       "      <td>22</td>\n",
       "      <td>0.929964</td>\n",
       "      <td>0.936165</td>\n",
       "      <td>0.931592</td>\n",
       "      <td>0.942005</td>\n",
       "      <td>0.941053</td>\n",
       "      <td>0.936156</td>\n",
       "      <td>0.004845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>4.667657</td>\n",
       "      <td>0.298610</td>\n",
       "      <td>0.597090</td>\n",
       "      <td>0.034242</td>\n",
       "      <td>median</td>\n",
       "      <td>33</td>\n",
       "      <td>22</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.786780</td>\n",
       "      <td>0.786893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786330</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>56</td>\n",
       "      <td>0.949999</td>\n",
       "      <td>0.953310</td>\n",
       "      <td>0.954039</td>\n",
       "      <td>0.956791</td>\n",
       "      <td>0.955726</td>\n",
       "      <td>0.953973</td>\n",
       "      <td>0.002335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>5.165572</td>\n",
       "      <td>0.473416</td>\n",
       "      <td>0.590605</td>\n",
       "      <td>0.054151</td>\n",
       "      <td>median</td>\n",
       "      <td>39</td>\n",
       "      <td>20</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.782179</td>\n",
       "      <td>0.790596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786554</td>\n",
       "      <td>0.003010</td>\n",
       "      <td>50</td>\n",
       "      <td>0.937148</td>\n",
       "      <td>0.946716</td>\n",
       "      <td>0.949494</td>\n",
       "      <td>0.943520</td>\n",
       "      <td>0.947842</td>\n",
       "      <td>0.944944</td>\n",
       "      <td>0.004360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>4.414008</td>\n",
       "      <td>0.184109</td>\n",
       "      <td>0.579093</td>\n",
       "      <td>0.027893</td>\n",
       "      <td>mean</td>\n",
       "      <td>38</td>\n",
       "      <td>23</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.781394</td>\n",
       "      <td>0.784648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.782245</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>98</td>\n",
       "      <td>0.965403</td>\n",
       "      <td>0.968405</td>\n",
       "      <td>0.971997</td>\n",
       "      <td>0.970399</td>\n",
       "      <td>0.969727</td>\n",
       "      <td>0.969186</td>\n",
       "      <td>0.002217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>4.386340</td>\n",
       "      <td>0.225394</td>\n",
       "      <td>0.565564</td>\n",
       "      <td>0.027599</td>\n",
       "      <td>mean</td>\n",
       "      <td>39</td>\n",
       "      <td>22</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.783077</td>\n",
       "      <td>0.786332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784040</td>\n",
       "      <td>0.002467</td>\n",
       "      <td>82</td>\n",
       "      <td>0.958024</td>\n",
       "      <td>0.964056</td>\n",
       "      <td>0.965656</td>\n",
       "      <td>0.964900</td>\n",
       "      <td>0.961029</td>\n",
       "      <td>0.962733</td>\n",
       "      <td>0.002831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>3.938500</td>\n",
       "      <td>0.084303</td>\n",
       "      <td>0.589668</td>\n",
       "      <td>0.023747</td>\n",
       "      <td>mean</td>\n",
       "      <td>34</td>\n",
       "      <td>22</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.786219</td>\n",
       "      <td>0.786219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787071</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>40</td>\n",
       "      <td>0.952187</td>\n",
       "      <td>0.953815</td>\n",
       "      <td>0.958978</td>\n",
       "      <td>0.957633</td>\n",
       "      <td>0.956288</td>\n",
       "      <td>0.955780</td>\n",
       "      <td>0.002477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>4.383955</td>\n",
       "      <td>0.096440</td>\n",
       "      <td>0.540168</td>\n",
       "      <td>0.008513</td>\n",
       "      <td>mean</td>\n",
       "      <td>43</td>\n",
       "      <td>20</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.787229</td>\n",
       "      <td>0.790259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787520</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>30</td>\n",
       "      <td>0.937681</td>\n",
       "      <td>0.943573</td>\n",
       "      <td>0.948371</td>\n",
       "      <td>0.943632</td>\n",
       "      <td>0.948909</td>\n",
       "      <td>0.944433</td>\n",
       "      <td>0.004062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>4.450534</td>\n",
       "      <td>0.202362</td>\n",
       "      <td>0.555855</td>\n",
       "      <td>0.012856</td>\n",
       "      <td>median</td>\n",
       "      <td>37</td>\n",
       "      <td>20</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.781057</td>\n",
       "      <td>0.788351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787856</td>\n",
       "      <td>0.003581</td>\n",
       "      <td>23</td>\n",
       "      <td>0.934426</td>\n",
       "      <td>0.941749</td>\n",
       "      <td>0.944836</td>\n",
       "      <td>0.939985</td>\n",
       "      <td>0.946019</td>\n",
       "      <td>0.941403</td>\n",
       "      <td>0.004096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>4.200018</td>\n",
       "      <td>0.424627</td>\n",
       "      <td>0.566384</td>\n",
       "      <td>0.024277</td>\n",
       "      <td>mean</td>\n",
       "      <td>33</td>\n",
       "      <td>24</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.788239</td>\n",
       "      <td>0.787117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787116</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>38</td>\n",
       "      <td>0.966076</td>\n",
       "      <td>0.969331</td>\n",
       "      <td>0.966638</td>\n",
       "      <td>0.970091</td>\n",
       "      <td>0.969474</td>\n",
       "      <td>0.968322</td>\n",
       "      <td>0.001634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>4.912700</td>\n",
       "      <td>0.188985</td>\n",
       "      <td>0.577651</td>\n",
       "      <td>0.028685</td>\n",
       "      <td>median</td>\n",
       "      <td>42</td>\n",
       "      <td>22</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.781282</td>\n",
       "      <td>0.791269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784804</td>\n",
       "      <td>0.003610</td>\n",
       "      <td>76</td>\n",
       "      <td>0.955863</td>\n",
       "      <td>0.961194</td>\n",
       "      <td>0.965235</td>\n",
       "      <td>0.966415</td>\n",
       "      <td>0.963582</td>\n",
       "      <td>0.962458</td>\n",
       "      <td>0.003734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>4.182076</td>\n",
       "      <td>0.146746</td>\n",
       "      <td>0.544129</td>\n",
       "      <td>0.012306</td>\n",
       "      <td>median</td>\n",
       "      <td>35</td>\n",
       "      <td>21</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.784312</td>\n",
       "      <td>0.784424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786173</td>\n",
       "      <td>0.002141</td>\n",
       "      <td>59</td>\n",
       "      <td>0.940290</td>\n",
       "      <td>0.948932</td>\n",
       "      <td>0.947670</td>\n",
       "      <td>0.944839</td>\n",
       "      <td>0.953229</td>\n",
       "      <td>0.946992</td>\n",
       "      <td>0.004305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>4.039918</td>\n",
       "      <td>0.074174</td>\n",
       "      <td>0.541868</td>\n",
       "      <td>0.004378</td>\n",
       "      <td>mean</td>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.779486</td>\n",
       "      <td>0.783975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784040</td>\n",
       "      <td>0.002461</td>\n",
       "      <td>82</td>\n",
       "      <td>0.964225</td>\n",
       "      <td>0.969752</td>\n",
       "      <td>0.975196</td>\n",
       "      <td>0.968884</td>\n",
       "      <td>0.971186</td>\n",
       "      <td>0.969848</td>\n",
       "      <td>0.003548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>3.725817</td>\n",
       "      <td>0.014960</td>\n",
       "      <td>0.538285</td>\n",
       "      <td>0.015464</td>\n",
       "      <td>mean</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.785995</td>\n",
       "      <td>0.781618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785163</td>\n",
       "      <td>0.003606</td>\n",
       "      <td>69</td>\n",
       "      <td>0.944050</td>\n",
       "      <td>0.943741</td>\n",
       "      <td>0.951373</td>\n",
       "      <td>0.950226</td>\n",
       "      <td>0.951855</td>\n",
       "      <td>0.948249</td>\n",
       "      <td>0.003595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>4.134176</td>\n",
       "      <td>0.061359</td>\n",
       "      <td>0.537055</td>\n",
       "      <td>0.006086</td>\n",
       "      <td>median</td>\n",
       "      <td>33</td>\n",
       "      <td>23</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.787566</td>\n",
       "      <td>0.789361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.788956</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>10</td>\n",
       "      <td>0.958781</td>\n",
       "      <td>0.958669</td>\n",
       "      <td>0.962962</td>\n",
       "      <td>0.964199</td>\n",
       "      <td>0.964648</td>\n",
       "      <td>0.961852</td>\n",
       "      <td>0.002612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>4.193267</td>\n",
       "      <td>0.096978</td>\n",
       "      <td>0.537711</td>\n",
       "      <td>0.006996</td>\n",
       "      <td>median</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.783526</td>\n",
       "      <td>0.780047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783928</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>84</td>\n",
       "      <td>0.961475</td>\n",
       "      <td>0.969163</td>\n",
       "      <td>0.970678</td>\n",
       "      <td>0.965966</td>\n",
       "      <td>0.968408</td>\n",
       "      <td>0.967138</td>\n",
       "      <td>0.003215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>3.884640</td>\n",
       "      <td>0.321675</td>\n",
       "      <td>0.543470</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>mean</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.783975</td>\n",
       "      <td>0.788239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786016</td>\n",
       "      <td>0.001933</td>\n",
       "      <td>64</td>\n",
       "      <td>0.946491</td>\n",
       "      <td>0.954937</td>\n",
       "      <td>0.958164</td>\n",
       "      <td>0.957745</td>\n",
       "      <td>0.955390</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.004221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>4.452993</td>\n",
       "      <td>0.215255</td>\n",
       "      <td>0.556966</td>\n",
       "      <td>0.017805</td>\n",
       "      <td>median</td>\n",
       "      <td>35</td>\n",
       "      <td>24</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.781169</td>\n",
       "      <td>0.781843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.782447</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>96</td>\n",
       "      <td>0.971436</td>\n",
       "      <td>0.972081</td>\n",
       "      <td>0.974719</td>\n",
       "      <td>0.968576</td>\n",
       "      <td>0.970625</td>\n",
       "      <td>0.971487</td>\n",
       "      <td>0.002001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>4.311751</td>\n",
       "      <td>0.070572</td>\n",
       "      <td>0.535543</td>\n",
       "      <td>0.014131</td>\n",
       "      <td>median</td>\n",
       "      <td>36</td>\n",
       "      <td>18</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.784312</td>\n",
       "      <td>0.789586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.788081</td>\n",
       "      <td>0.003055</td>\n",
       "      <td>18</td>\n",
       "      <td>0.904627</td>\n",
       "      <td>0.915991</td>\n",
       "      <td>0.921519</td>\n",
       "      <td>0.912264</td>\n",
       "      <td>0.911453</td>\n",
       "      <td>0.913171</td>\n",
       "      <td>0.005557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>4.031415</td>\n",
       "      <td>0.102598</td>\n",
       "      <td>0.538015</td>\n",
       "      <td>0.014914</td>\n",
       "      <td>median</td>\n",
       "      <td>32</td>\n",
       "      <td>18</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.790933</td>\n",
       "      <td>0.785209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789338</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>9</td>\n",
       "      <td>0.906451</td>\n",
       "      <td>0.906226</td>\n",
       "      <td>0.917394</td>\n",
       "      <td>0.915940</td>\n",
       "      <td>0.906122</td>\n",
       "      <td>0.910426</td>\n",
       "      <td>0.005117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>3.713830</td>\n",
       "      <td>0.023250</td>\n",
       "      <td>0.538665</td>\n",
       "      <td>0.014705</td>\n",
       "      <td>mean</td>\n",
       "      <td>33</td>\n",
       "      <td>22</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.787341</td>\n",
       "      <td>0.790371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787811</td>\n",
       "      <td>0.001768</td>\n",
       "      <td>26</td>\n",
       "      <td>0.953815</td>\n",
       "      <td>0.958332</td>\n",
       "      <td>0.956200</td>\n",
       "      <td>0.960214</td>\n",
       "      <td>0.959542</td>\n",
       "      <td>0.957621</td>\n",
       "      <td>0.002342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>4.490773</td>\n",
       "      <td>0.069051</td>\n",
       "      <td>0.565562</td>\n",
       "      <td>0.019991</td>\n",
       "      <td>median</td>\n",
       "      <td>43</td>\n",
       "      <td>21</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.788800</td>\n",
       "      <td>0.793514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787654</td>\n",
       "      <td>0.003413</td>\n",
       "      <td>28</td>\n",
       "      <td>0.947473</td>\n",
       "      <td>0.955947</td>\n",
       "      <td>0.961868</td>\n",
       "      <td>0.953929</td>\n",
       "      <td>0.956708</td>\n",
       "      <td>0.955185</td>\n",
       "      <td>0.004661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>4.612352</td>\n",
       "      <td>0.278362</td>\n",
       "      <td>0.590348</td>\n",
       "      <td>0.064899</td>\n",
       "      <td>median</td>\n",
       "      <td>37</td>\n",
       "      <td>24</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.779935</td>\n",
       "      <td>0.785995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785477</td>\n",
       "      <td>0.003824</td>\n",
       "      <td>67</td>\n",
       "      <td>0.968995</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.973428</td>\n",
       "      <td>0.971297</td>\n",
       "      <td>0.969390</td>\n",
       "      <td>0.971571</td>\n",
       "      <td>0.002236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>4.170682</td>\n",
       "      <td>0.052006</td>\n",
       "      <td>0.541625</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>mean</td>\n",
       "      <td>42</td>\n",
       "      <td>22</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.779710</td>\n",
       "      <td>0.778813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785342</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>68</td>\n",
       "      <td>0.954320</td>\n",
       "      <td>0.966582</td>\n",
       "      <td>0.968433</td>\n",
       "      <td>0.967173</td>\n",
       "      <td>0.964171</td>\n",
       "      <td>0.964136</td>\n",
       "      <td>0.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>4.360225</td>\n",
       "      <td>0.033442</td>\n",
       "      <td>0.566669</td>\n",
       "      <td>0.034439</td>\n",
       "      <td>median</td>\n",
       "      <td>40</td>\n",
       "      <td>18</td>\n",
       "      <td>{'simpleimputer__strategy': 'median', 'selectk...</td>\n",
       "      <td>0.783863</td>\n",
       "      <td>0.789249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.788350</td>\n",
       "      <td>0.003359</td>\n",
       "      <td>16</td>\n",
       "      <td>0.908359</td>\n",
       "      <td>0.923286</td>\n",
       "      <td>0.922445</td>\n",
       "      <td>0.923066</td>\n",
       "      <td>0.926098</td>\n",
       "      <td>0.920651</td>\n",
       "      <td>0.006273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>3.718861</td>\n",
       "      <td>0.095729</td>\n",
       "      <td>0.551245</td>\n",
       "      <td>0.006817</td>\n",
       "      <td>mean</td>\n",
       "      <td>34</td>\n",
       "      <td>21</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.787678</td>\n",
       "      <td>0.791269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787228</td>\n",
       "      <td>0.002739</td>\n",
       "      <td>37</td>\n",
       "      <td>0.940935</td>\n",
       "      <td>0.946688</td>\n",
       "      <td>0.954432</td>\n",
       "      <td>0.949552</td>\n",
       "      <td>0.951855</td>\n",
       "      <td>0.948692</td>\n",
       "      <td>0.004645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>4.095857</td>\n",
       "      <td>0.063665</td>\n",
       "      <td>0.539326</td>\n",
       "      <td>0.013966</td>\n",
       "      <td>mean</td>\n",
       "      <td>43</td>\n",
       "      <td>18</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.788464</td>\n",
       "      <td>0.787903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786465</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>52</td>\n",
       "      <td>0.919891</td>\n",
       "      <td>0.921575</td>\n",
       "      <td>0.923567</td>\n",
       "      <td>0.923319</td>\n",
       "      <td>0.929521</td>\n",
       "      <td>0.923575</td>\n",
       "      <td>0.003256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>4.089818</td>\n",
       "      <td>0.067708</td>\n",
       "      <td>0.541224</td>\n",
       "      <td>0.007282</td>\n",
       "      <td>mean</td>\n",
       "      <td>43</td>\n",
       "      <td>19</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.782404</td>\n",
       "      <td>0.790484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787116</td>\n",
       "      <td>0.003261</td>\n",
       "      <td>38</td>\n",
       "      <td>0.927748</td>\n",
       "      <td>0.932153</td>\n",
       "      <td>0.938242</td>\n",
       "      <td>0.938245</td>\n",
       "      <td>0.934684</td>\n",
       "      <td>0.934214</td>\n",
       "      <td>0.003969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>4.155231</td>\n",
       "      <td>0.202315</td>\n",
       "      <td>0.526097</td>\n",
       "      <td>0.031504</td>\n",
       "      <td>mean</td>\n",
       "      <td>42</td>\n",
       "      <td>23</td>\n",
       "      <td>{'simpleimputer__strategy': 'mean', 'selectkbe...</td>\n",
       "      <td>0.783077</td>\n",
       "      <td>0.790371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783636</td>\n",
       "      <td>0.004331</td>\n",
       "      <td>86</td>\n",
       "      <td>0.964730</td>\n",
       "      <td>0.969556</td>\n",
       "      <td>0.970622</td>\n",
       "      <td>0.971269</td>\n",
       "      <td>0.969306</td>\n",
       "      <td>0.969096</td>\n",
       "      <td>0.002297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        3.619744      0.182049         0.493774        0.015340   \n",
       "1        4.536158      0.104687         0.529449        0.015119   \n",
       "2        4.512257      0.178875         0.532015        0.009846   \n",
       "3        4.413463      0.040377         0.541752        0.019198   \n",
       "4        4.407448      0.122330         0.543203        0.016223   \n",
       "5        3.807020      0.136344         0.551015        0.028082   \n",
       "6        4.276196      0.076805         0.534697        0.009830   \n",
       "7        4.026540      0.093005         0.536502        0.010497   \n",
       "8        4.535480      0.072411         0.537862        0.001967   \n",
       "9        4.136824      0.121963         0.541445        0.005696   \n",
       "10       3.837650      0.047664         0.548320        0.017399   \n",
       "11       3.896381      0.111690         0.545593        0.012163   \n",
       "12       3.756494      0.083364         0.538632        0.007623   \n",
       "13       4.270776      0.098646         0.547203        0.007464   \n",
       "14       4.167279      0.413744         0.540980        0.007797   \n",
       "15       4.079068      0.097265         0.537157        0.007789   \n",
       "16       4.554545      0.081526         0.546218        0.009657   \n",
       "17       4.145304      0.051479         0.550549        0.014929   \n",
       "18       3.843284      0.072202         0.544886        0.010765   \n",
       "19       3.789494      0.054912         0.544174        0.007697   \n",
       "20       4.054221      0.054029         0.599293        0.069121   \n",
       "21       4.406868      0.191238         0.547975        0.011324   \n",
       "22       4.141467      0.115484         0.544680        0.009700   \n",
       "23       4.298831      0.236699         0.578854        0.041553   \n",
       "24       3.965623      0.035553         0.554278        0.006278   \n",
       "25       5.449733      0.783896         0.547931        0.012573   \n",
       "26       4.286354      0.199870         0.558261        0.023635   \n",
       "27       4.227367      0.082260         0.565833        0.016955   \n",
       "28       4.231173      0.052836         0.578269        0.034020   \n",
       "29       4.440418      0.065937         0.590479        0.020410   \n",
       "..            ...           ...              ...             ...   \n",
       "70       4.030811      0.366559         0.590226        0.076757   \n",
       "71       4.256714      0.124252         0.597869        0.059610   \n",
       "72       4.780695      0.381782         0.623917        0.047306   \n",
       "73       4.667657      0.298610         0.597090        0.034242   \n",
       "74       5.165572      0.473416         0.590605        0.054151   \n",
       "75       4.414008      0.184109         0.579093        0.027893   \n",
       "76       4.386340      0.225394         0.565564        0.027599   \n",
       "77       3.938500      0.084303         0.589668        0.023747   \n",
       "78       4.383955      0.096440         0.540168        0.008513   \n",
       "79       4.450534      0.202362         0.555855        0.012856   \n",
       "80       4.200018      0.424627         0.566384        0.024277   \n",
       "81       4.912700      0.188985         0.577651        0.028685   \n",
       "82       4.182076      0.146746         0.544129        0.012306   \n",
       "83       4.039918      0.074174         0.541868        0.004378   \n",
       "84       3.725817      0.014960         0.538285        0.015464   \n",
       "85       4.134176      0.061359         0.537055        0.006086   \n",
       "86       4.193267      0.096978         0.537711        0.006996   \n",
       "87       3.884640      0.321675         0.543470        0.020619   \n",
       "88       4.452993      0.215255         0.556966        0.017805   \n",
       "89       4.311751      0.070572         0.535543        0.014131   \n",
       "90       4.031415      0.102598         0.538015        0.014914   \n",
       "91       3.713830      0.023250         0.538665        0.014705   \n",
       "92       4.490773      0.069051         0.565562        0.019991   \n",
       "93       4.612352      0.278362         0.590348        0.064899   \n",
       "94       4.170682      0.052006         0.541625        0.014526   \n",
       "95       4.360225      0.033442         0.566669        0.034439   \n",
       "96       3.718861      0.095729         0.551245        0.006817   \n",
       "97       4.095857      0.063665         0.539326        0.013966   \n",
       "98       4.089818      0.067708         0.541224        0.007282   \n",
       "99       4.155231      0.202315         0.526097        0.031504   \n",
       "\n",
       "   param_simpleimputer__strategy param_selectkbest__k  \\\n",
       "0                           mean                   38   \n",
       "1                         median                   40   \n",
       "2                         median                   43   \n",
       "3                         median                   39   \n",
       "4                         median                   38   \n",
       "5                           mean                   32   \n",
       "6                           mean                   42   \n",
       "7                         median                   31   \n",
       "8                         median                   42   \n",
       "9                           mean                   39   \n",
       "10                          mean                   30   \n",
       "11                          mean                   35   \n",
       "12                          mean                   30   \n",
       "13                          mean                   42   \n",
       "14                          mean                   34   \n",
       "15                          mean                   39   \n",
       "16                        median                   40   \n",
       "17                        median                   31   \n",
       "18                          mean                   33   \n",
       "19                          mean                   32   \n",
       "20                          mean                   38   \n",
       "21                        median                   36   \n",
       "22                        median                   31   \n",
       "23                          mean                   36   \n",
       "24                          mean                   35   \n",
       "25                        median                   42   \n",
       "26                        median                   35   \n",
       "27                        median                   30   \n",
       "28                          mean                   40   \n",
       "29                        median                   37   \n",
       "..                           ...                  ...   \n",
       "70                          mean                   33   \n",
       "71                          mean                   30   \n",
       "72                        median                   34   \n",
       "73                        median                   33   \n",
       "74                        median                   39   \n",
       "75                          mean                   38   \n",
       "76                          mean                   39   \n",
       "77                          mean                   34   \n",
       "78                          mean                   43   \n",
       "79                        median                   37   \n",
       "80                          mean                   33   \n",
       "81                        median                   42   \n",
       "82                        median                   35   \n",
       "83                          mean                   36   \n",
       "84                          mean                   30   \n",
       "85                        median                   33   \n",
       "86                        median                   32   \n",
       "87                          mean                   31   \n",
       "88                        median                   35   \n",
       "89                        median                   36   \n",
       "90                        median                   32   \n",
       "91                          mean                   33   \n",
       "92                        median                   43   \n",
       "93                        median                   37   \n",
       "94                          mean                   42   \n",
       "95                        median                   40   \n",
       "96                          mean                   34   \n",
       "97                          mean                   43   \n",
       "98                          mean                   43   \n",
       "99                          mean                   42   \n",
       "\n",
       "   param_randomforestclassifier__max_depth  \\\n",
       "0                                       21   \n",
       "1                                       20   \n",
       "2                                       18   \n",
       "3                                       21   \n",
       "4                                       23   \n",
       "5                                       19   \n",
       "6                                       24   \n",
       "7                                       19   \n",
       "8                                       20   \n",
       "9                                       21   \n",
       "10                                      23   \n",
       "11                                      24   \n",
       "12                                      18   \n",
       "13                                      20   \n",
       "14                                      20   \n",
       "15                                      19   \n",
       "16                                      24   \n",
       "17                                      21   \n",
       "18                                      23   \n",
       "19                                      22   \n",
       "20                                      19   \n",
       "21                                      24   \n",
       "22                                      20   \n",
       "23                                      22   \n",
       "24                                      18   \n",
       "25                                      21   \n",
       "26                                      19   \n",
       "27                                      24   \n",
       "28                                      19   \n",
       "29                                      23   \n",
       "..                                     ...   \n",
       "70                                      18   \n",
       "71                                      19   \n",
       "72                                      20   \n",
       "73                                      22   \n",
       "74                                      20   \n",
       "75                                      23   \n",
       "76                                      22   \n",
       "77                                      22   \n",
       "78                                      20   \n",
       "79                                      20   \n",
       "80                                      24   \n",
       "81                                      22   \n",
       "82                                      21   \n",
       "83                                      24   \n",
       "84                                      21   \n",
       "85                                      23   \n",
       "86                                      24   \n",
       "87                                      22   \n",
       "88                                      24   \n",
       "89                                      18   \n",
       "90                                      18   \n",
       "91                                      22   \n",
       "92                                      21   \n",
       "93                                      24   \n",
       "94                                      22   \n",
       "95                                      18   \n",
       "96                                      21   \n",
       "97                                      18   \n",
       "98                                      19   \n",
       "99                                      23   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'simpleimputer__strategy': 'mean', 'selectkbe...           0.777915   \n",
       "1   {'simpleimputer__strategy': 'median', 'selectk...           0.781955   \n",
       "2   {'simpleimputer__strategy': 'median', 'selectk...           0.784424   \n",
       "3   {'simpleimputer__strategy': 'median', 'selectk...           0.787566   \n",
       "4   {'simpleimputer__strategy': 'median', 'selectk...           0.777915   \n",
       "5   {'simpleimputer__strategy': 'mean', 'selectkbe...           0.788464   \n",
       "6   {'simpleimputer__strategy': 'mean', 'selectkbe...           0.780720   \n",
       "7   {'simpleimputer__strategy': 'median', 'selectk...           0.787790   \n",
       "8   {'simpleimputer__strategy': 'median', 'selectk...           0.779374   \n",
       "9   {'simpleimputer__strategy': 'mean', 'selectkbe...           0.779374   \n",
       "10  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.783526   \n",
       "11  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.783077   \n",
       "12  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.788239   \n",
       "13  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.783638   \n",
       "14  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.791830   \n",
       "15  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.786107   \n",
       "16  {'simpleimputer__strategy': 'median', 'selectk...           0.781169   \n",
       "17  {'simpleimputer__strategy': 'median', 'selectk...           0.786444   \n",
       "18  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.790259   \n",
       "19  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.786444   \n",
       "20  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.783302   \n",
       "21  {'simpleimputer__strategy': 'median', 'selectk...           0.779374   \n",
       "22  {'simpleimputer__strategy': 'median', 'selectk...           0.785995   \n",
       "23  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.781730   \n",
       "24  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.781618   \n",
       "25  {'simpleimputer__strategy': 'median', 'selectk...           0.783077   \n",
       "26  {'simpleimputer__strategy': 'median', 'selectk...           0.784199   \n",
       "27  {'simpleimputer__strategy': 'median', 'selectk...           0.786556   \n",
       "28  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.787117   \n",
       "29  {'simpleimputer__strategy': 'median', 'selectk...           0.783302   \n",
       "..                                                ...                ...   \n",
       "70  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.788239   \n",
       "71  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.790708   \n",
       "72  {'simpleimputer__strategy': 'median', 'selectk...           0.788464   \n",
       "73  {'simpleimputer__strategy': 'median', 'selectk...           0.786780   \n",
       "74  {'simpleimputer__strategy': 'median', 'selectk...           0.782179   \n",
       "75  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.781394   \n",
       "76  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.783077   \n",
       "77  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.786219   \n",
       "78  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.787229   \n",
       "79  {'simpleimputer__strategy': 'median', 'selectk...           0.781057   \n",
       "80  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.788239   \n",
       "81  {'simpleimputer__strategy': 'median', 'selectk...           0.781282   \n",
       "82  {'simpleimputer__strategy': 'median', 'selectk...           0.784312   \n",
       "83  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.779486   \n",
       "84  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.785995   \n",
       "85  {'simpleimputer__strategy': 'median', 'selectk...           0.787566   \n",
       "86  {'simpleimputer__strategy': 'median', 'selectk...           0.783526   \n",
       "87  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.783975   \n",
       "88  {'simpleimputer__strategy': 'median', 'selectk...           0.781169   \n",
       "89  {'simpleimputer__strategy': 'median', 'selectk...           0.784312   \n",
       "90  {'simpleimputer__strategy': 'median', 'selectk...           0.790933   \n",
       "91  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.787341   \n",
       "92  {'simpleimputer__strategy': 'median', 'selectk...           0.788800   \n",
       "93  {'simpleimputer__strategy': 'median', 'selectk...           0.779935   \n",
       "94  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.779710   \n",
       "95  {'simpleimputer__strategy': 'median', 'selectk...           0.783863   \n",
       "96  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.787678   \n",
       "97  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.788464   \n",
       "98  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.782404   \n",
       "99  {'simpleimputer__strategy': 'mean', 'selectkbe...           0.783077   \n",
       "\n",
       "    split1_test_score  ...  mean_test_score  std_test_score  rank_test_score  \\\n",
       "0            0.791943  ...         0.784691        0.004445               78   \n",
       "1            0.794299  ...         0.786914        0.004039               43   \n",
       "2            0.788239  ...         0.786779        0.001540               46   \n",
       "3            0.784873  ...         0.785791        0.001121               66   \n",
       "4            0.784648  ...         0.781235        0.002358              100   \n",
       "5            0.793850  ...         0.790101        0.001929                6   \n",
       "6            0.783863  ...         0.782918        0.003613               95   \n",
       "7            0.791718  ...         0.788799        0.002206               12   \n",
       "8            0.790933  ...         0.787452        0.004221               34   \n",
       "9            0.782179  ...         0.784489        0.003415               79   \n",
       "10           0.787341  ...         0.787003        0.001824               41   \n",
       "11           0.781618  ...         0.782020        0.001462               99   \n",
       "12           0.793289  ...         0.789764        0.001827                7   \n",
       "13           0.788688  ...         0.786734        0.002714               48   \n",
       "14           0.795309  ...         0.791448        0.002673                1   \n",
       "15           0.787903  ...         0.787475        0.001857               33   \n",
       "16           0.793289  ...         0.787520        0.003871               30   \n",
       "17           0.783975  ...         0.785948        0.002273               65   \n",
       "18           0.786668  ...         0.786667        0.001909               49   \n",
       "19           0.781394  ...         0.786958        0.004085               42   \n",
       "20           0.786107  ...         0.785140        0.001291               71   \n",
       "21           0.783189  ...         0.783322        0.002363               90   \n",
       "22           0.788800  ...         0.787946        0.002249               20   \n",
       "23           0.785097  ...         0.784848        0.002972               75   \n",
       "24           0.787117  ...         0.786824        0.003011               45   \n",
       "25           0.785209  ...         0.786083        0.001975               62   \n",
       "26           0.788239  ...         0.787924        0.003103               21   \n",
       "27           0.782404  ...         0.784759        0.003320               77   \n",
       "28           0.788015  ...         0.787520        0.001483               30   \n",
       "29           0.782404  ...         0.783120        0.002439               93   \n",
       "..                ...  ...              ...             ...              ...   \n",
       "70           0.793514  ...         0.790168        0.001879                4   \n",
       "71           0.788351  ...         0.789675        0.000913                8   \n",
       "72           0.790035  ...         0.787879        0.002240               22   \n",
       "73           0.786893  ...         0.786330        0.001152               56   \n",
       "74           0.790596  ...         0.786554        0.003010               50   \n",
       "75           0.784648  ...         0.782245        0.001672               98   \n",
       "76           0.786332  ...         0.784040        0.002467               82   \n",
       "77           0.786219  ...         0.787071        0.000817               40   \n",
       "78           0.790259  ...         0.787520        0.002107               30   \n",
       "79           0.788351  ...         0.787856        0.003581               23   \n",
       "80           0.787117  ...         0.787116        0.000710               38   \n",
       "81           0.791269  ...         0.784804        0.003610               76   \n",
       "82           0.784424  ...         0.786173        0.002141               59   \n",
       "83           0.783975  ...         0.784040        0.002461               82   \n",
       "84           0.781618  ...         0.785163        0.003606               69   \n",
       "85           0.789361  ...         0.788956        0.001618               10   \n",
       "86           0.780047  ...         0.783928        0.002353               84   \n",
       "87           0.788239  ...         0.786016        0.001933               64   \n",
       "88           0.781843  ...         0.782447        0.001582               96   \n",
       "89           0.789586  ...         0.788081        0.003055               18   \n",
       "90           0.785209  ...         0.789338        0.002086                9   \n",
       "91           0.790371  ...         0.787811        0.001768               26   \n",
       "92           0.793514  ...         0.787654        0.003413               28   \n",
       "93           0.785995  ...         0.785477        0.003824               67   \n",
       "94           0.778813  ...         0.785342        0.005075               68   \n",
       "95           0.789249  ...         0.788350        0.003359               16   \n",
       "96           0.791269  ...         0.787228        0.002739               37   \n",
       "97           0.787903  ...         0.786465        0.002305               52   \n",
       "98           0.790484  ...         0.787116        0.003261               38   \n",
       "99           0.790371  ...         0.783636        0.004331               86   \n",
       "\n",
       "    split0_train_score  split1_train_score  split2_train_score  \\\n",
       "0             0.949325            0.951289            0.957518   \n",
       "1             0.938747            0.948483            0.947445   \n",
       "2             0.917562            0.927664            0.921126   \n",
       "3             0.949830            0.953955            0.957687   \n",
       "4             0.962429            0.968181            0.970173   \n",
       "5             0.915654            0.923511            0.928000   \n",
       "6             0.970257            0.975813            0.977553   \n",
       "7             0.911417            0.923651            0.927579   \n",
       "8             0.945032            0.943573            0.950728   \n",
       "9             0.940935            0.954713            0.956312   \n",
       "10            0.956087            0.958921            0.963158   \n",
       "11            0.968798            0.974550            0.972839   \n",
       "12            0.898566            0.908527            0.912540   \n",
       "13            0.937765            0.947417            0.952159   \n",
       "14            0.927692            0.937540            0.941637   \n",
       "15            0.924661            0.932995            0.939112   \n",
       "16            0.970005            0.974494            0.974382   \n",
       "17            0.939364            0.937400            0.949662   \n",
       "18            0.956677            0.964056            0.963383   \n",
       "19            0.949578            0.958052            0.951879   \n",
       "20            0.922893            0.924409            0.936755   \n",
       "21            0.967115            0.968798            0.974803   \n",
       "22            0.924437            0.930946            0.937232   \n",
       "23            0.958360            0.961419            0.962008   \n",
       "24            0.898398            0.914560            0.922893   \n",
       "25            0.949914            0.963046            0.959651   \n",
       "26            0.922865            0.929796            0.929010   \n",
       "27            0.963804            0.962345            0.966161   \n",
       "28            0.920396            0.933584            0.933444   \n",
       "29            0.959483            0.970005            0.967844   \n",
       "..                 ...                 ...                 ...   \n",
       "70            0.905693            0.908752            0.916496   \n",
       "71            0.919807            0.916720            0.933219   \n",
       "72            0.929964            0.936165            0.931592   \n",
       "73            0.949999            0.953310            0.954039   \n",
       "74            0.937148            0.946716            0.949494   \n",
       "75            0.965403            0.968405            0.971997   \n",
       "76            0.958024            0.964056            0.965656   \n",
       "77            0.952187            0.953815            0.958978   \n",
       "78            0.937681            0.943573            0.948371   \n",
       "79            0.934426            0.941749            0.944836   \n",
       "80            0.966076            0.969331            0.966638   \n",
       "81            0.955863            0.961194            0.965235   \n",
       "82            0.940290            0.948932            0.947670   \n",
       "83            0.964225            0.969752            0.975196   \n",
       "84            0.944050            0.943741            0.951373   \n",
       "85            0.958781            0.958669            0.962962   \n",
       "86            0.961475            0.969163            0.970678   \n",
       "87            0.946491            0.954937            0.958164   \n",
       "88            0.971436            0.972081            0.974719   \n",
       "89            0.904627            0.915991            0.921519   \n",
       "90            0.906451            0.906226            0.917394   \n",
       "91            0.953815            0.958332            0.956200   \n",
       "92            0.947473            0.955947            0.961868   \n",
       "93            0.968995            0.974747            0.973428   \n",
       "94            0.954320            0.966582            0.968433   \n",
       "95            0.908359            0.923286            0.922445   \n",
       "96            0.940935            0.946688            0.954432   \n",
       "97            0.919891            0.921575            0.923567   \n",
       "98            0.927748            0.932153            0.938242   \n",
       "99            0.964730            0.969556            0.970622   \n",
       "\n",
       "    split3_train_score  split4_train_score  mean_train_score  std_train_score  \n",
       "0             0.950226            0.956680          0.953008         0.003408  \n",
       "1             0.950478            0.948965          0.946824         0.004155  \n",
       "2             0.918661            0.920291          0.921061         0.003527  \n",
       "3             0.956090            0.956877          0.954888         0.002817  \n",
       "4             0.965545            0.968492          0.966964         0.002709  \n",
       "5             0.931540            0.924976          0.924736         0.005310  \n",
       "6             0.975562            0.975983          0.975034         0.002488  \n",
       "7             0.928229            0.922591          0.922694         0.006043  \n",
       "8             0.944979            0.949891          0.946841         0.002892  \n",
       "9             0.957016            0.956175          0.953030         0.006094  \n",
       "10            0.962655            0.964648          0.961094         0.003134  \n",
       "11            0.970399            0.967763          0.970870         0.002514  \n",
       "12            0.914284            0.906515          0.908086         0.005505  \n",
       "13            0.946438            0.947506          0.946257         0.004688  \n",
       "14            0.938919            0.941025          0.937362         0.005053  \n",
       "15            0.935832            0.936227          0.933765         0.004948  \n",
       "16            0.975253            0.976460          0.974119         0.002187  \n",
       "17            0.946943            0.947590          0.944192         0.004868  \n",
       "18            0.963918            0.962292          0.962065         0.002765  \n",
       "19            0.962347            0.958925          0.956156         0.004715  \n",
       "20            0.928229            0.934880          0.929433         0.005527  \n",
       "21            0.967594            0.968296          0.969321         0.002801  \n",
       "22            0.933896            0.933505          0.932003         0.004279  \n",
       "23            0.955136            0.955698          0.958524         0.002829  \n",
       "24            0.913302            0.916166          0.913064         0.008045  \n",
       "25            0.959036            0.959907          0.958311         0.004422  \n",
       "26            0.932325            0.922535          0.927306         0.003919  \n",
       "27            0.965405            0.963386          0.964220         0.001382  \n",
       "28            0.932045            0.935217          0.930937         0.005365  \n",
       "29            0.961870            0.966556          0.965152         0.003891  \n",
       "..                 ...                 ...               ...              ...  \n",
       "70            0.908140            0.916250          0.911066         0.004453  \n",
       "71            0.926124            0.921525          0.923479         0.005742  \n",
       "72            0.942005            0.941053          0.936156         0.004845  \n",
       "73            0.956791            0.955726          0.953973         0.002335  \n",
       "74            0.943520            0.947842          0.944944         0.004360  \n",
       "75            0.970399            0.969727          0.969186         0.002217  \n",
       "76            0.964900            0.961029          0.962733         0.002831  \n",
       "77            0.957633            0.956288          0.955780         0.002477  \n",
       "78            0.943632            0.948909          0.944433         0.004062  \n",
       "79            0.939985            0.946019          0.941403         0.004096  \n",
       "80            0.970091            0.969474          0.968322         0.001634  \n",
       "81            0.966415            0.963582          0.962458         0.003734  \n",
       "82            0.944839            0.953229          0.946992         0.004305  \n",
       "83            0.968884            0.971186          0.969848         0.003548  \n",
       "84            0.950226            0.951855          0.948249         0.003595  \n",
       "85            0.964199            0.964648          0.961852         0.002612  \n",
       "86            0.965966            0.968408          0.967138         0.003215  \n",
       "87            0.957745            0.955390          0.954545         0.004221  \n",
       "88            0.968576            0.970625          0.971487         0.002001  \n",
       "89            0.912264            0.911453          0.913171         0.005557  \n",
       "90            0.915940            0.906122          0.910426         0.005117  \n",
       "91            0.960214            0.959542          0.957621         0.002342  \n",
       "92            0.953929            0.956708          0.955185         0.004661  \n",
       "93            0.971297            0.969390          0.971571         0.002236  \n",
       "94            0.967173            0.964171          0.964136         0.005100  \n",
       "95            0.923066            0.926098          0.920651         0.006273  \n",
       "96            0.949552            0.951855          0.948692         0.004645  \n",
       "97            0.923319            0.929521          0.923575         0.003256  \n",
       "98            0.938245            0.934684          0.934214         0.003969  \n",
       "99            0.971269            0.969306          0.969096         0.002297  \n",
       "\n",
       "[100 rows x 23 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results=pd.DataFrame(search.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_results=results.sort_values(by='rank_test_score').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79    {'randomforestclassifier__max_depth': 20, 'ran...\n",
       "83    {'randomforestclassifier__max_depth': 17, 'ran...\n",
       "94    {'randomforestclassifier__max_depth': 14, 'ran...\n",
       "12    {'randomforestclassifier__max_depth': 20, 'ran...\n",
       "19    {'randomforestclassifier__max_depth': 23, 'ran...\n",
       "Name: params, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_results.params.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'simpleimputer__strategy': 'mean', 'selectkbest__k': 38, 'randomforestclassifier__max_depth': 21}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_params() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-0e7fd2cb67a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: set_params() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "pipeline=make_pipeline(\n",
    "    ce.OrdinalEncoder(),\n",
    "    SimpleImputer(),\n",
    "    SelectKBest(),\n",
    "    RandomForestClassifier()\n",
    "\n",
    ") \n",
    "for x in results.params.head(10):\n",
    "    print(x)\n",
    "    pipeline.set_params(dict(x))\n",
    "    pipeline.train(train_x, train_y)\n",
    "    pipeline.score(val_x,val_y)\n",
    "    print(x,pipeline.score(val_x,val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Pipeline.get_params of Pipeline(memory=None,\n",
       "     steps=[('ordinalencoder', OrdinalEncoder(cols=None, drop_invariant=False, handle_unknown='impute',\n",
       "        impute_missing=True, mapping=None, return_df=True, verbose=0)), ('simpleimputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy='mean',\n",
       "       verbose=0)), ('selectkbes...obs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
